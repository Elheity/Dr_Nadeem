{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elheity/Dr_Nadeem/blob/main/comp_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "import os\n",
        "import itertools\n",
        "import sklearn.metrics\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import gray2rgb\n",
        "import matplotlib.pyplot as plt\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-01-26T14:05:39.031996Z",
          "iopub.execute_input": "2023-01-26T14:05:39.033055Z",
          "iopub.status.idle": "2023-01-26T14:05:39.995848Z",
          "shell.execute_reply.started": "2023-01-26T14:05:39.032983Z",
          "shell.execute_reply": "2023-01-26T14:05:39.994877Z"
        },
        "trusted": true,
        "id": "KGAaNnNdB7Tl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "GTg8A9hTCSYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paths = []\n",
        "labels = []\n",
        "def create_paths_labels(path):\n",
        "    for root,dirs,files in os.walk(path):\n",
        "        for pics in files:\n",
        "            #print(os.path.join(root, pics))\n",
        "            paths.append(os.path.join(root, pics))\n",
        "            if os.path.basename(root) == 1:\n",
        "                labels.append(1)\n",
        "            elif os.path.basename(root) == 0:\n",
        "                labels.append(0)\n",
        "    return [paths,labels]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-26T13:29:42.736094Z",
          "iopub.execute_input": "2023-01-26T13:29:42.736549Z",
          "iopub.status.idle": "2023-01-26T13:29:42.745473Z",
          "shell.execute_reply.started": "2023-01-26T13:29:42.736517Z",
          "shell.execute_reply": "2023-01-26T13:29:42.743140Z"
        },
        "trusted": true,
        "id": "ANf1wZ7HB7Tm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths = []\n",
        "labels = []\n",
        "path = r\"/content/drive/MyDrive/train\"\n",
        "for root,dirs,files in os.walk(path):\n",
        "        for pics in files:\n",
        "            #print(os.path.join(root, pics))\n",
        "            paths.append(os.path.join(root, pics))\n",
        "            #print( 'x',dirs)\n",
        "            if str(os.path.basename(root)) == '1':\n",
        "                labels.append(1)\n",
        "                #print( 'y',root)\n",
        "            elif str(os.path.basename(root)) == '0':\n",
        "                labels.append(0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-26T14:05:50.807545Z",
          "iopub.execute_input": "2023-01-26T14:05:50.807943Z",
          "iopub.status.idle": "2023-01-26T14:06:08.217150Z",
          "shell.execute_reply.started": "2023-01-26T14:05:50.807900Z",
          "shell.execute_reply": "2023-01-26T14:06:08.216108Z"
        },
        "trusted": true,
        "id": "zZ72HEYXB7Tn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X6c0rtlCe-H",
        "outputId": "b578e2bf-7f18-4283-8708-24dc584974d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(paths)\n",
        "paths = paths[:-1]\n",
        "paths[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TLtvS9YBJAwG",
        "outputId": "f261ddc4-aa47-4a0f-ba59-2ffa120033b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/train/1/41584_1735459739.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-26T13:48:46.607087Z",
          "iopub.execute_input": "2023-01-26T13:48:46.607558Z",
          "iopub.status.idle": "2023-01-26T13:48:46.619568Z",
          "shell.execute_reply.started": "2023-01-26T13:48:46.607496Z",
          "shell.execute_reply": "2023-01-26T13:48:46.618404Z"
        },
        "trusted": true,
        "id": "B6S759s1B7To"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "imgs = []\n",
        "for imgph in paths:\n",
        "    img = io.imread(imgph)\n",
        "    i += 1\n",
        "    img_resized = resize(img ,(128,128))\n",
        "    imgs.append(img_resized)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-26T14:09:31.148719Z",
          "iopub.execute_input": "2023-01-26T14:09:31.149192Z",
          "iopub.status.idle": "2023-01-26T14:15:41.585541Z",
          "shell.execute_reply.started": "2023-01-26T14:09:31.149154Z",
          "shell.execute_reply": "2023-01-26T14:15:41.584222Z"
        },
        "trusted": true,
        "id": "SjDR82HdB7Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(imgs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2guJNPWJ1J0",
        "outputId": "ae7e5f92-1976-44f9-eb18-82e64f2a4d03"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x =[]\n",
        "# Create a data generator object\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "# Load an image from file\n",
        "for i in paths:\n",
        "  img = image.load_img(i, target_size=(150, 150))\n",
        "  # Convert the image to a numpy array\n",
        "  x = image.img_to_array(img)\n",
        "  # Reshape the array to add a dimension for the batch size\n",
        "  x = x.reshape((1,) + x.shape)\n",
        "\n",
        "  # Generate and save augmented images\n",
        "  i = 0\n",
        "  for batch in datagen.flow(x, batch_size=1, save_to_dir='/content/preview', save_prefix='img', save_format='jpeg'):\n",
        "      i += 1\n",
        "      if i > 20:\n",
        "          break"
      ],
      "metadata": {
        "id": "2NGnshNwpkiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DoWLwUbD_WLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_path_toImage(paths):\n",
        "    imgs = []\n",
        "    for path in paths:\n",
        "        img = io.imread(path)\n",
        "        img = resize(img , 128,128)\n",
        "        \n",
        "    return imgs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-26T13:40:16.880708Z",
          "iopub.execute_input": "2023-01-26T13:40:16.881054Z",
          "iopub.status.idle": "2023-01-26T13:40:16.891176Z",
          "shell.execute_reply.started": "2023-01-26T13:40:16.881024Z",
          "shell.execute_reply": "2023-01-26T13:40:16.890151Z"
        },
        "trusted": true,
        "id": "0-YNmM0KB7Tp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_imgs = create_paths_labels(r\"/kaggle/input/mammography-breast-cancer-detection/train\")\n",
        "#train_imgs = convert_path_toImage(train_imgs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-26T13:40:16.893107Z",
          "iopub.execute_input": "2023-01-26T13:40:16.893481Z",
          "iopub.status.idle": "2023-01-26T13:40:16.905639Z",
          "shell.execute_reply.started": "2023-01-26T13:40:16.893425Z",
          "shell.execute_reply": "2023-01-26T13:40:16.904375Z"
        },
        "trusted": true,
        "id": "JCbt6F7VB7Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-26T14:25:27.784443Z",
          "iopub.execute_input": "2023-01-26T14:25:27.784869Z",
          "iopub.status.idle": "2023-01-26T14:25:27.793643Z",
          "shell.execute_reply.started": "2023-01-26T14:25:27.784836Z",
          "shell.execute_reply": "2023-01-26T14:25:27.792508Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "rE76lSTWB7Tq",
        "outputId": "26397a73-70ed-4e6b-97fc-64c9d2e9d312"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a7f4a5366567>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_imgs = np.array(imgs)\n",
        "train_labels = np.array(labels)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-26T14:25:29.002303Z",
          "iopub.execute_input": "2023-01-26T14:25:29.002691Z"
        },
        "trusted": true,
        "id": "Sd5FA8rVB7Tq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = train_labels[:-1]"
      ],
      "metadata": {
        "id": "a07HCzqnqWGB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_imgs))\n",
        "print(len(train_imgs.shape))\n",
        "print(len(train_labels))\n",
        "train_imgs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaqwY9wUB7Tq",
        "outputId": "291ea6e3-8589-45df-efb9-f084ac46c0e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11449\n",
            "3\n",
            "11449\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11449, 128, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid, X_train = train_imgs[:1500] / 255.0, train_imgs[1500:] / 255.0\n",
        "y_valid, y_train = train_labels[:1500], train_labels[1500:]"
      ],
      "metadata": {
        "id": "Vv4Q7boIqx5B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXhl-puqtYX5",
        "outputId": "47a4c74a-b109-46f2-9ad9-1e55eb6a18ab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9949, 128, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VY0s-Hwb_YTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the hypermatarest we would test and their range\n",
        "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([ 0.3, 0.4]))\n",
        "# Defining the hyperparameters we would tune, and their values to be tested\n",
        "HP_FILTER_SIZE = hp.HParam('filter_size', hp.Discrete([3 , 5]))\n",
        "HP_FILTER_NUM = hp.HParam('filters_number', hp.Discrete([64, 128,192]))\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "# Logging setup info\n",
        "with tf.summary.create_file_writer(r'Logs/Model 1/hparam_tuning/').as_default():\n",
        "    hp.hparams_config(\n",
        "        hparams=[HP_DROPOUT],\n",
        "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
        "    )"
      ],
      "metadata": {
        "id": "8yoQ0eEhnuAR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 15\n",
        "BATCH_SIZE = 40"
      ],
      "metadata": {
        "id": "o-623tean3gR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the generator\n",
        "train_gen = ImageDataGenerator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "XnOcCJpX_jES",
        "outputId": "ea5dc2c9-1d7f-4c30-a1bb-95589839d8f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-753cebb79e5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapping our model and training in a function\n",
        "def train_test_model(hparams, session_num):\n",
        "    global model\n",
        "    # Outlining the model/architecture of our CNN\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(hparams[HP_FILTER_NUM], hparams[HP_FILTER_SIZE],\n",
        "                               activation='relu',\n",
        "                               padding=\"same\",\n",
        "                               input_shape=(128,128,1)),\n",
        "        tf.keras.layers.MaxPooling2D(2),\n",
        "        tf.keras.layers.Conv2D(hparams[HP_FILTER_NUM], 3, padding=\"same\", activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2),\n",
        "        #tf.keras.layers.Conv2D(hparams[HP_FILTER_NUM], 3, padding=\"same\", activation='relu'),\n",
        "        #tf.keras.layers.MaxPooling2D(2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        #tf.keras.layers.Dense(1024, activation='relu'),\n",
        "        #tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
        "        tf.keras.layers.Dense(2, activation=\"softmax\")\n",
        "    ])\n",
        "    #model.summary()\n",
        "    # Defining the loss function\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    # use generator to train\n",
        "    model.fit_generator(train_gen.flow(X_train, y_train, batch_size=32),\n",
        "                        steps_per_epoch=len(X_train) / 32, epochs=10)\n",
        "    # Compiling the model with parameter value for the optimizer\n",
        "    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "    def plot_confusion_matrix(cm, class_names):\n",
        "        \"\"\"\n",
        "        Returns a matplotlib figure containing the plotted confusion matrix.\n",
        "\n",
        "        Args:\n",
        "          cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
        "          class_names (array, shape = [n]): String names of the integer classes\n",
        "        \"\"\"\n",
        "        figure = plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "        plt.title(\"Confusion matrix\")\n",
        "        plt.colorbar()\n",
        "        tick_marks = np.arange(len(class_names))\n",
        "        plt.xticks(tick_marks, class_names, rotation=45)\n",
        "        plt.yticks(tick_marks, class_names)\n",
        "\n",
        "        # Normalize the confusion matrix.\n",
        "        cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "\n",
        "        # Use white text if squares are dark; otherwise black.\n",
        "        threshold = cm.max() / 2.\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            color = \"white\" if cm[i, j] > threshold else \"black\"\n",
        "            plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "        return figure\n",
        "    \n",
        "    \n",
        "    # Logging the training process data to use later in tensorboard\n",
        "    log_dir = \"Logs\\\\Model 6\\\\fit\\\\\" + \"run-{}\".format(session_num)\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)\n",
        "    file_writer_cm = tf.summary.create_file_writer(log_dir + '/cm')  \n",
        "    \n",
        "    \n",
        "    def plot_to_image(figure):\n",
        "        \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
        "        returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
        "        # Save the plot to a PNG in memory.\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format='png')\n",
        "        # Closing the figure prevents it from being displayed directly inside\n",
        "        # the notebook.\n",
        "        plt.close(figure)\n",
        "        buf.seek(0)\n",
        "        # Convert PNG buffer to TF image\n",
        "        image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "        # Add the batch dimension\n",
        "        image = tf.expand_dims(image, 0)\n",
        "        return image\n",
        "    \n",
        "    def log_confusion_matrix(epoch, logs):\n",
        "        # Use the model to predict the values from the validation dataset.\n",
        "        test_pred_raw = model.predict(X_valid)\n",
        "        test_pred = np.argmax(test_pred_raw, axis=1)\n",
        "\n",
        "        # Calculate the confusion matrix.\n",
        "        cm = sklearn.metrics.confusion_matrix(y_valid, test_pred)\n",
        "        # Log the confusion matrix as an image summary.\n",
        "        figure = plot_confusion_matrix(cm, class_names = [\"yes\", \"no\"])\n",
        "        cm_image = plot_to_image(figure)\n",
        "\n",
        "        # Log the confusion matrix as an image summary.\n",
        "        with file_writer_cm.as_default():\n",
        "            tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\n",
        "    \n",
        "    \n",
        "    # Define the per-epoch callback.\n",
        "    cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
        "\n",
        "    \n",
        "    # Defining early stopping to prevent overfitting\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor = 'val_loss',\n",
        "        mode = 'auto',\n",
        "        min_delta = 0,\n",
        "        patience = 2,\n",
        "        verbose = 0, \n",
        "        restore_best_weights = True\n",
        "    )\n",
        "    \n",
        "    # Training the model\n",
        "    model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs = EPOCHS,\n",
        "        batch_size = BATCH_SIZE,\n",
        "        callbacks = [tensorboard_callback, cm_callback, early_stopping],\n",
        "        validation_data = (X_valid,y_valid),\n",
        "        verbose = 2\n",
        "    )\n",
        "    \n",
        "    _, accuracy = model.evaluate(X_valid,y_valid)\n",
        "    \n",
        "    model.save(r\"saved_models\\Model 1\\Run-{}\".format(session_num))\n",
        "    \n",
        "    \n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "F6GSJVA4n5xy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yhvd0Cw_suyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(images_test, y_test)\n",
        "print('////////////////////////////////// Testing Part /////////////////////////////////////')\n",
        "print('////////////////////////////////////////////////////////////////////////////////////')\n",
        "print('Test loss: {0:.4f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
      ],
      "metadata": {
        "id": "vWXcV1eopPUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function to log the resuls\n",
        "def run(log_dir, hparams, session_num):\n",
        "    \n",
        "    with tf.summary.create_file_writer(log_dir).as_default():\n",
        "        hp.hparams(hparams)  # record the values used in this trial\n",
        "        accuracy = train_test_model(hparams, session_num)\n",
        "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
      ],
      "metadata": {
        "id": "hKzzzgzQrZv3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_num = 1\n",
        "\n",
        "for filter_size in HP_FILTER_SIZE.domain.values:\n",
        "    for filter_num in HP_FILTER_NUM.domain.values:\n",
        "        for dropout in HP_DROPOUT.domain.values:\n",
        "\n",
        "            hparams = {\n",
        "                HP_FILTER_SIZE: filter_size,\n",
        "                HP_FILTER_NUM: filter_num,\n",
        "                HP_DROPOUT: dropout\n",
        "            }\n",
        "\n",
        "            run_name = \"run-%d\" % session_num\n",
        "            print('--- Starting trial: %s' % run_name)\n",
        "            print({h.name: hparams[h] for h in hparams})\n",
        "            run('Logs/Model 1/hparam_tuning/' + run_name, hparams, session_num)\n",
        "\n",
        "            session_num += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "3txW66Z-oMrQ",
        "outputId": "f7fb0457-ca79-4d54-8bbf-41fa397c7161"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-17eba93d4088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msession_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilter_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHP_FILTER_SIZE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilter_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHP_FILTER_NUM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHP_DROPOUT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'HP_FILTER_SIZE' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function to log the resuls\n",
        "def run(log_dir, hparams, session_num):\n",
        "    \n",
        "    with tf.summary.create_file_writer(log_dir).as_default():\n",
        "        hp.hparams(hparams)  # record the values used in this trial\n",
        "        accuracy = train_test_model(hparams, session_num)\n",
        "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
      ],
      "metadata": {
        "id": "MxJqwQIcoChJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "def identity_block(X, f, filters, stage, block):\n",
        "    \"\"\"\n",
        "    Implementation of the identity block\n",
        "    \"\"\"\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value. You'll need this later to add back to the main path. \n",
        "    X_shortcut = X\n",
        "    \n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CBW19Ot1KPQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4giRCs3OKzNQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}